Grok Master Docs

==============


# Note By Note - AI Practice Plan Generator for Music Teachers

## Overview
An AI-powered mobile and web app for music teachers to record lessons and automatically generate student recaps, practice plans, and parent emails. This MVP focuses on efficiency, reducing admin time from hours to seconds, with a sleek dark-mode UI. Built for solo teachers ($49/month) and studios ($99/month), targeting 100,000+ US private music instructors.

**Core Features**:
- Audio Recording: Simple one-tap interface.
- AI Processing: Whisper for transcription + LLM for extraction and generation.
- 3-Output Generation: Student Recap, Practice Plan, Parent Email.
- Review Mode: Edit outputs before sending.
- Historical Storage: Database of past lessons and students.

**Tech Stack** (Pragmatic for MVP - Unified codebase, serverless where possible):
| Component | Technology | Reasoning |
|-----------|------------|-----------|
| Frontend (Mobile + Web) | React Native (via Expo) | Single codebase for iOS and Web; Expo for easy deployment and navigation. |
| UI Library | Tamagui (Tailwind for RN) | Sleek, minimalist styling with dark mode support. |
| Backend API | FastAPI (Python) | High performance, native AI library support. |
| Database | PostgreSQL (via Supabase) | Managed DB with auth, storage, and realtime; handles user tiers and lesson history. |
| AI Services | OpenAI Whisper (Transcription) + GPT-4o (Generation) | Aligns with PDF: Tuned templates for teacher-like outputs. |
| Testing | Pytest (Backend), Jest (Frontend) | 80%+ coverage with mocks and golden fixtures. |
| CI/CD | GitHub Actions | Automated testing, coverage enforcement. |
| DevOps | Supabase CLI, Expo EAS | Quick setup, no infra management. |

**Development Standards**:
- Testing: >80% coverage; all PRs must pass.
- UI: Dark mode default; minimalist typography (e.g., Inter font, 16px base).
- Code Style: PEP8 (Python), Prettier (JS/TS).
- Latency: Transcription + Generation <60s for standard lessons.
- Security: Supabase Row Level Security (RLS) for user data isolation.

**Monorepo Structure** (For AI Agents - Extend this scaffold):
```
note-by-note/
├── backend/                  # FastAPI service
│   ├── main.py               # Core app with endpoints
│   ├── test_main.py          # Tests with mocks
│   ├── requirements.txt      # Dependencies
│   └── prompts/              # LLM system prompts
├── frontend/                 # React Native app
│   ├── App.js                # Entry point
│   ├── screens/              # RecordScreen.js, ReviewScreen.js
│   ├── services/             # api.js for backend calls
│   └── package.json          # Dependencies
├── shared/                   # Shared types, schemas
│   ├── database.ts           # Prisma-like TypeScript types
│   └── lesson.schema.json    # JSON Schema for outputs
├── tests/                    # Golden fixtures and runners
│   ├── golden/               # Fixture files (transcript + expected)
│   └── run_regression.py     # Pipeline tester
├── prisma/                   # Database schema
│   └── schema.prisma         # Models and enums
├── .github/workflows/        # CI/CD YAML
├── README.md                 # This file
└── Makefile                  # Quick commands (e.g., make test)
```

**Setup Instructions** (For Agents - Run Locally):
1. Clone repo.
2. Backend: `cd backend; pip install -r requirements.txt; uvicorn main:app --reload`.
3. Frontend: `cd frontend; yarn install; expo start`.
4. Database: `supabase start` (local) or deploy to Supabase.
5. Env: Add `.env` with `OPENAI_API_KEY` and `SUPABASE_URL/KEY`.
6. Test: `make test` (Runs backend/frontend tests + regression).

## Product Requirements Document (PRD)

### Problem Statement
Music teachers spend hours post-lesson writing notes, plans, and emails, leading to burnout. The PDF describes a teacher at 9pm still planning for 23 students, emailing parents by 11pm.

### Solution Overview
Mobile/web app: Record lesson → AI transcribes → Extracts musical details → Generates 3 editable outputs → Send/store.

### User Personas
- **Solo Teacher (Sarah)**: Independent instructor; needs quick efficiency; $49/month.
- **Studio Owner (Michael)**: Manages multiple teachers/students; needs multi-user support; $99/month.

### Core Functional Requirements (MVP)
| ID | Feature | Description | Acceptance Criteria |
|----|---------|-------------|----------------------|
| REC-001 | Audio Capture | One-tap recording on mobile/web. | Starts in <500ms; handles up to 60min; saves as .m4a. |
| TRX-001 | AI Transcription | Upload audio to backend; use Whisper. | Returns text transcript; <30s for 10min audio. |
| GEN-001 | Intelligence Layer | LLM extracts: mistakes, techniques, pieces, tempos. | Outputs JSON with extracted fields; accuracy >90% on test fixtures. |
| GEN-002 | Output Generation | Generate 3 docs: Recap (encouraging wins/areas), Plan (daily breakdowns), Email (progress summary). | Matches teacher tone; editable; uses templates. |
| REV-001 | Review Flow | Display/edit outputs before send. | UI shows 3 sections; saves edits to DB. |
| STO-001 | Historical Storage | Store lessons, students, outputs. | Queryable by user; soft deletes. |
| AUT-001 | User Auth | Supabase auth for sign-up/login. | Supports tiers; limits lessons/month (e.g., 100 for Solo). |

### Non-Functional Requirements
- UI/UX: Dark mode; minimalist (e.g., hex #1A1A1A background, #FFFFFF text).
- Performance: End-to-end <60s.
- Security: RLS on DB; encrypt audio in storage.
- Accessibility: WCAG 2.1 compliant.
- Scalability: Handle 850 users (per PDF target for $500K ARR).

### User Flows (ASCII Diagram)
```
Home → Student List → Select Student → Record Screen (Tap Record)
          ↓ (Stop Recording)
Upload → Process (Transcription → Extraction → Generation)
          ↓
Review Screen (Edit Recap/Plan/Email) → Send/Store
          ↓
History View (Past Lessons)
```

## System Architecture
High-Level Flow:
```
Frontend (Expo) → API (FastAPI) → AI (OpenAI) → DB (Supabase)
- Audio upload via multipart.
- Async processing if >10min (MVP: sync).
- Realtime updates via Supabase subscriptions.
```

## Database Schema (Prisma Format - Use with Supabase)
```prisma
datasource db {
  provider = "postgresql"
  url      = env("DATABASE_URL")
}

generator client {
  provider = "prisma-client-js"
}

enum Plan {
  SOLO
  STUDIO
}

enum Instrument {
  PIANO
  VIOLIN
  // ... (full list from Epsilon)
}

enum LessonStatus {
  CREATED
  RECORDING
  UPLOADING
  TRANSCRIBING
  GENERATING
  COMPLETED
  FAILED
}

enum OutputType {
  STUDENT_RECAP
  PRACTICE_PLAN
  PARENT_EMAIL
}

model User {
  id              String    @id @default(uuid())
  email           String    @unique
  firstName       String?
  lastName        String?
  plan            Plan      @default(SOLO)
  lessonsThisMonth Int      @default(0)
  createdAt       DateTime  @default(now())
  updatedAt       DateTime  @updatedAt
  students        Student[]
  lessons         Lesson[]
  settings        Settings?
}

model Student {
  id            String      @id @default(uuid())
  userId        String
  firstName     String
  lastName      String
  parentEmail   String
  instrument    Instrument
  // ... (full fields from Epsilon)
  user          User        @relation(fields: [userId], references: [id], onDelete: Cascade)
  lessons       Lesson[]
}

model Lesson {
  id            String        @id @default(uuid())
  userId        String
  studentId     String
  audioUrl      String?
  status        LessonStatus  @default(CREATED)
  // ... (full fields)
  user          User          @relation(fields: [userId], references: [id], onDelete: Cascade)
  student       Student       @relation(fields: [studentId], references: [id], onDelete: Cascade)
  transcript    Transcript?
  outputs       Output[]
}

model Transcript {
  id          String    @id @default(uuid())
  lessonId    String    @unique
  fullText    String
  // ... (segments, etc.)
  lesson      Lesson    @relation(fields: [lessonId], references: [id], onDelete: Cascade)
}

model Output {
  id            String      @id @default(uuid())
  lessonId      String
  type          OutputType
  content       String
  editedContent String?
  // ... 
  lesson        Lesson      @relation(fields: [lessonId], references: [id], onDelete: Cascade)
}

// Additional models: Settings, AuditLog (from Epsilon)
```

**Migrations & Seed**: Include SQL for full-text search (from Epsilon). Seed with demo user/students.

## API Specs (OpenAPI Style)
Endpoint: POST /process-lesson
- Request: Multipart file (audio).
- Response: JSON { transcript: str, outputs: { recap: str, plan: str, email: str } }
- Errors: 500 on failure; include codes.

Other Endpoints: /users, /students, /lessons (CRUD with auth).

## AI Prompts & Schemas
**JSON Schema (lesson.schema.json - For Validation)**:
```json
{
  "type": "object",
  "properties": {
    "student_recap": { "type": "string" },
    "practice_plan": { "type": "string" },
    "parent_email": { "type": "string" }
  },
  "required": ["student_recap", "practice_plan", "parent_email"]
}
```

**System Prompt (prompts/music_teacher_generation.txt)**:
```
ROLE: Expert music teacher assistant.
INPUT: Lesson transcript.
TASK: Extract mistakes, techniques, pieces, tempos. Generate:
1. STUDENT_RECAP: Encouraging, specific wins/areas.
2. PRACTICE_PLAN: Daily format e.g., "Monday: C major scales, 10min."
3. PARENT_EMAIL: Professional summary, nudges.
CONSTRAINT: Human-like tone; JSON output.
```

## Code Scaffolds
### Backend (backend/main.py)
```python
from fastapi import FastAPI, UploadFile, HTTPException
from openai import OpenAI
import os, tempfile, shutil
from pydantic import BaseModel

app = FastAPI()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

class LessonOutputs(BaseModel):
    student_recap: str
    practice_plan: str
    parent_email: str

def transcribe_audio(file_path: str) -> str:
    with open(file_path, "rb") as f:
        return client.audio.transcriptions.create(model="whisper-1", file=f).text

def generate_outputs(transcript: str) -> LessonOutputs:
    # Use prompt above; parse JSON response
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "system", "content": SYSTEM_PROMPT}, {"role": "user", "content": transcript}],
        response_format={"type": "json_object"}
    )
    content = response.choices[0].message.content
    return LessonOutputs.parse_raw(content)  # Validate with schema

@app.post("/process-lesson")
async def process_lesson(file: UploadFile):
    with tempfile.NamedTemporaryFile(suffix=".m4a") as temp:
        shutil.copyfileobj(file.file, temp)
        transcript = transcribe_audio(temp.name)
        outputs = generate_outputs(transcript)
    return {"transcript": transcript, "outputs": outputs}
```

### Frontend (frontend/screens/RecordScreen.js)
```javascript
import React, { useState } from 'react';
import { View, TouchableOpacity, Text } from 'react-native';
import { Audio } from 'expo-av';
import { processLessonAudio } from '../services/api';

export default function RecordScreen({ navigation }) {
  const [recording, setRecording] = useState(null);

  async function startRecording() {
    const { recording } = await Audio.Recording.createAsync(Audio.RecordingOptionsPresets.HIGH_QUALITY);
    setRecording(recording);
  }

  async function stopRecording() {
    await recording.stopAndUnloadAsync();
    const uri = recording.getURI();
    const result = await processLessonAudio(uri);
    navigation.navigate('Review', { data: result });
  }

  return (
    <View style={{ flex: 1, backgroundColor: '#1A1A1A', justifyContent: 'center' }}>
      <TouchableOpacity onPress={recording ? stopRecording : startRecording}>
        <Text style={{ color: '#FFF', fontSize: 24 }}>{recording ? 'Stop' : 'Record'}</Text>
      </TouchableOpacity>
    </View>
  );
}
```

// Additional: ReviewScreen.js with editable text areas.

## Testing Strategy
- **Unit/Integration**: Pytest for backend (mocks OpenAI); Jest for frontend.
- **Golden Fixtures**: From Beta - transcripts + expected outputs; run_regression.py validates pipeline.
- **Coverage**: .coveragerc enforces 80%.
- **E2E**: Simulate upload → process → validate schema.

**Example Test (test_main.py)**:
```python
import pytest
from fastapi.testclient import TestClient
from unittest.mock import patch
from main import app

client = TestClient(app)

@patch('main.client.audio.transcriptions.create')
@patch('main.client.chat.completions.create')
def test_process_lesson(mock_chat, mock_whisper):
    mock_whisper.return_value.text = "Mock transcript"
    mock_chat.return_value.choices[0].message.content = '{"student_recap": "Win!", "practice_plan": "Monday: Scales", "parent_email": "Progress!"}'
    response = client.post("/process-lesson", files={'file': ('test.m4a', b'fake', 'audio/m4a')})
    assert response.status_code == 200
    assert 'outputs' in response.json()
```

## CI/CD ( .github/workflows/ci.yml )
```yaml
name: CI
on: [push, pull_request]
jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Backend Tests
        run: cd backend; pip install -r requirements.txt; pytest --cov=. --cov-fail-under=80
      - name: Frontend Tests
        run: cd frontend; yarn install; yarn test
      - name: Regression
        run: python tests/run_regression.py
```

## AI Agent Implementation Steps
1. **Database Agent**: Deploy Prisma schema to Supabase; add RLS.
2. **Backend Agent**: Extend main.py with DB integration (prisma-py or SQLAlchemy).
3. **Frontend Agent**: Add student list, history screens; wire to API.
4. **QA Agent**: Generate more golden fixtures; tune prompts until 95% match on real lessons.
5. **Tune & Validate**: Compare outputs to manual teacher notes (per PDF); iterate.

This consolidated system provides executable scaffolds, strict contracts, comprehensive specs, and quality gates for a perfect MVP build.