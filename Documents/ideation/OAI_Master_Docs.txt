OAI Master Docs
===============

```text
notebynote/
  README.md
  AGENTS.md
  Makefile
  .env.example
  .gitignore
  .github/workflows/ci.yml

  docs/
    INDEX.md
    PRD.md
    ARCHITECTURE.md
    API.md
    DB_SCHEMA.sql
    AI_CONTRACT.md
    DEPLOYMENT.md
    RUNBOOK.md
    SECURITY_PRIVACY.md
    PAYMENTS.md

  supabase/
    migrations/
      001_init.sql

  packages/
    ai_contract/
      README.md
      schema/
        lesson_extraction.schema.json
        outputs.schema.json
      prompts/
        extraction.md
        student_recap.md
        practice_plan.md
        parent_email.md
      fixtures/
        golden/
          fixture_0001/
            transcript.txt
            expected_extraction.json
            expected_outputs.json
      src/
        __init__.py
        adapters.py
        runner.py
        validate.py
      tests/
        test_golden_fixtures.py

  services/
    api/
      pyproject.toml
      app/
        __init__.py
        main.py
        settings.py
        auth.py
        errors.py
        db.py
        models.py
        routes/
          __init__.py
          health.py
          students.py
          lessons.py
          outputs.py
        services/
          __init__.py
          openai_client.py
          ai_pipeline.py
          emailer.py
          storage.py
      tests/
        test_health.py
        test_pipeline_contract.py

  apps/
    mobile/
      package.json
      app.json
      tsconfig.json
      src/
        App.tsx
        lib/
          api.ts
          supabase.ts
        screens/
          RecordScreen.tsx
          ReviewScreen.tsx
```

---

```markdown
===== FILE: README.md =====
# Note By Note

Note By Note is a cross platform MVP for music teachers that records a lesson, transcribes it, and generates three editable outputs:
- Student recap
- Practice plan
- Parent email

This is the core workflow described in the idea document.  

## Stack decisions

Single stack only, to keep AI agents from hallucinating architecture:
- Mobile plus web: Expo React Native
- Backend: FastAPI Python
- DB plus auth plus storage: Supabase Postgres plus Supabase Auth plus Supabase Storage
- AI: OpenAI Whisper for transcription plus an LLM for extraction and generation

Why this is non negotiable:
- Agents ship faster with one canonical stack and one source of truth.
- The AI layer must be treated like a component with a contract, not magic text generation.

## Monorepo layout

- apps/mobile: Expo app
- services/api: FastAPI API and pipeline orchestrator
- packages/ai_contract: schema first AI contract, prompts, fixtures, deterministic runner

## Quick start

Prereqs:
- Node 20+
- Python 3.12+
- Supabase project created

1. Copy env template
   cp .env.example .env

2. Install Python deps and run API
   make api-install
   make api-dev

3. Install mobile deps and run Expo
   make mobile-install
   make mobile-dev

## Commands

- make test: run all tests and contract checks
- make fmt: format Python and TypeScript
- make lint: lint Python and TypeScript
- make ci: run the same checks as GitHub Actions

## Definition of Done

A feature is Done only if:
- It matches acceptance criteria in docs/PRD.md
- API and DB changes are reflected in docs/API.md and docs/DB_SCHEMA.sql
- Contract tests and golden fixtures remain green
- Coverage stays at or above 80 percent for Python

See AGENTS.md for the multi agent workflow.
```

---

```markdown
===== FILE: AGENTS.md =====
# AI Agent Operating Manual

This repo is designed for a team of AI software engineering agents.

## Canonical sources of truth

1. docs/PRD.md
2. docs/API.md
3. supabase/migrations/001_init.sql
4. packages/ai_contract/schema/*.json
5. packages/ai_contract/fixtures/golden/*

If anything conflicts, update the docs and migrations first, then update code.

## Required workflow

1. Contract first
   - Any change to extraction or output shape must start by editing JSON Schemas in packages/ai_contract/schema.
   - Then update fixtures or add a new fixture.

2. API second
   - Update docs/API.md and Pydantic models.
   - Keep response shapes stable.

3. DB third
   - Update supabase/migrations with forward only SQL.
   - Add RLS policy changes explicitly.

4. Code last
   - Implement smallest possible change that satisfies acceptance criteria.
   - Add tests for negative cases.

## Hard rules

- Do not introduce a second backend, second frontend, or native Swift app.
- Do not rename DB columns casually.
- Do not change AI schemas without updating fixtures.
- No untracked prompt edits. Prompts live in packages/ai_contract/prompts.

## Branching tasks

Recommended agent roles:
- Product Agent: maintains PRD acceptance criteria
- Backend Agent: FastAPI routes and pipeline
- Data Agent: Supabase migrations and RLS
- AI Quality Agent: golden fixtures, contract tests, prompt tuning
- Frontend Agent: Expo screens and UX polish

## What makes this 10 out of 10 for agents

- Single stack, no branching architecture choices.
- Machine readable schemas plus golden fixtures prevent silent regressions.
- Runnable scaffold prevents blank page paralysis.
```

---

```markdown
===== FILE: docs/INDEX.md =====
# Docs index

- PRD: docs/PRD.md
- Architecture: docs/ARCHITECTURE.md
- API contract: docs/API.md
- Database schema reference: docs/DB_SCHEMA.sql
- AI contract and tuning: docs/AI_CONTRACT.md
- Security and privacy: docs/SECURITY_PRIVACY.md
- Deployment: docs/DEPLOYMENT.md
- Runbook: docs/RUNBOOK.md
- Payments strategy: docs/PAYMENTS.md
```

---

```markdown
===== FILE: docs/PRD.md =====
# Product Requirements Document

## Problem

Music teachers spend unpaid time after lessons writing lesson notes, practice plans, and parent updates.

## MVP goal

After a lesson, the teacher can:
1. Record or upload audio
2. Generate three editable outputs
3. Save them to a student timeline
4. Copy or send the parent email

## Personas

- Solo Teacher
  - Wants speed and minimal admin
  - Pricing target 49 per month

- Studio Owner
  - Needs multiple students and staff
  - Pricing target 99 per month

## MVP scope

In scope:
- Email magic link sign in
- Student CRUD
- Record or upload audio
- Async processing job with status
- Transcript stored
- AI extraction and three outputs stored
- Edit outputs and mark as sent
- History list per student
- Basic analytics: time to generate, edits made

Out of scope for MVP:
- Stripe paywall enforcement in app
- Multi teacher organization roles
- iPad specific layout
- Multi language

## Non functional requirements

- Sleek and clean UI
- Dark mode first
- Latency target under 60 seconds for a standard segment
- Minimum 80 percent test coverage for backend and contract suite

## Feature requirements with IDs

### Auth

AUTH-001 Magic link sign in
Acceptance criteria:
- User can request a magic link
- User can sign in and get a valid session
- API endpoints reject missing or invalid auth token

### Students

STU-001 Create student
Acceptance criteria:
- Create student with name and optional parent email
- Student belongs to current user

STU-002 List students
Acceptance criteria:
- Lists only students owned by current user
- Sorted by most recent lesson

### Recording and upload

REC-001 Start recording
Acceptance criteria:
- Recording starts within 500 ms on device
- Shows elapsed time and stop button

REC-002 Stop and upload
Acceptance criteria:
- A single action stops recording and starts upload
- Upload progress visible

### Processing

JOB-001 Create processing job
Acceptance criteria:
- After upload, lesson status becomes TRANSCRIBING then GENERATING then READY
- Status endpoint returns progress and error message if failed

TRX-001 Transcription
Acceptance criteria:
- Transcript stored with timestamps when available
- Failure is retryable

### AI generation

GEN-001 Student recap
Acceptance criteria:
- 150 to 300 words
- Includes what went well and focus areas

GEN-002 Practice plan
Acceptance criteria:
- 7 day plan with specific assignments
- Includes measurable targets

GEN-003 Parent email
Acceptance criteria:
- Professional tone
- Ready to paste or send

### Output editing

OUT-001 Edit output
Acceptance criteria:
- User can edit content and save
- Original content remains stored

OUT-002 Mark as sent
Acceptance criteria:
- Output can be marked sent with method clipboard or email
- Timestamp recorded
```

---

```markdown
===== FILE: docs/ARCHITECTURE.md =====
# Architecture

## High level flow

1. Mobile app uploads audio to Supabase Storage
2. Mobile app calls API to start processing for a lesson
3. API creates a job record and runs pipeline
4. Pipeline steps:
   - Transcribe audio with Whisper
   - Extract structured lesson facts using JSON Schema
   - Generate three outputs using constrained prompts
   - Store outputs and mark lesson READY

## Core principle

Treat the LLM like a component with a contract:
- Schema first extraction
- Template constrained generation
- Golden fixtures plus CI gates

## Components

- Expo app
  - Auth and data reads through Supabase client
  - Calls API only for processing and email send

- FastAPI service
  - Verifies Supabase user token
  - Orchestrates AI pipeline and writes results to DB

- Supabase
  - Postgres tables
  - Storage bucket for audio
  - Auth for sessions

## Error model

- Every pipeline step can fail with a typed error code
- Retries are allowed for transcription and generation
- If generation fails after retries, lesson becomes FAILED and UI shows retry

## Why not edge functions only

Edge functions can work, but FastAPI keeps AI libs and test harness straightforward for agents and supports the contract suite cleanly.
```

---

```markdown
===== FILE: docs/API.md =====
# API contract

Base URL:
- Local: http://localhost:8000

Auth:
- Bearer token from Supabase session access token

Common response envelope:
{
  "success": true,
  "data": ...
}

Errors:
{
  "success": false,
  "error": {
    "code": "AUTH_INVALID",
    "message": "..."
  }
}

## Health

GET /health
Response:
{ "success": true, "data": { "status": "ok" } }

## Students

GET /v1/students
POST /v1/students
PATCH /v1/students/{student_id}
DELETE /v1/students/{student_id}

## Lessons and processing

POST /v1/lessons
Body:
{
  "studentId": "uuid",
  "title": "Optional title",
  "audioStoragePath": "storage key in Supabase"
}

Response:
{
  "success": true,
  "data": {
    "lessonId": "uuid",
    "status": "QUEUED"
  }
}

GET /v1/lessons/{lesson_id}
GET /v1/lessons/{lesson_id}/status

POST /v1/lessons/{lesson_id}/retry
Body:
{ "fromStep": "TRANSCRIBE" | "EXTRACT" | "GENERATE" }

## Outputs

GET /v1/outputs/{output_id}
PATCH /v1/outputs/{output_id}
Body:
{ "editedContent": "..." }

POST /v1/outputs/{output_id}/sent
Body:
{ "sentTo": "email", "sentVia": "clipboard" | "email" }

POST /v1/outputs/{output_id}/send-email
Body:
{ "to": "email" }
Notes:
- Uses Resend if configured
- Falls back to returning a mailto payload
```

---

```sql
===== FILE: docs/DB_SCHEMA.sql =====
-- Reference schema, actual source of truth is supabase/migrations/001_init.sql

-- profiles: 1 per auth user
-- students: belongs to profile
-- lessons: belongs to student plus profile
-- outputs: 3 per lesson
-- jobs: pipeline status and retry tracking
```

---

```markdown
===== FILE: docs/AI_CONTRACT.md =====
# AI contract

Goal:
- Convert transcript into structured lesson extraction JSON
- Generate three outputs using that JSON, not raw transcript

This reduces hallucination and makes outputs testable.

## Extraction schema

Canonical schema:
- packages/ai_contract/schema/lesson_extraction.schema.json

Evidence requirement:
- Every extracted claim must include a short supporting quote from transcript where possible.
- Confidence must be a float 0 to 1.

## Output schema

Canonical schema:
- packages/ai_contract/schema/outputs.schema.json

## Golden fixtures

Folder:
- packages/ai_contract/fixtures/golden

Each fixture contains:
- transcript.txt
- expected_extraction.json
- expected_outputs.json

Rule:
- If prompts change, fixtures must be updated intentionally.
- CI runs the deterministic adapter for fixture validation and schema checks.

## Tuning loop

1. Run pilot with real teachers
2. Capture transcript and teacher edited outputs
3. Create a fixture from the transcript and final teacher approved outputs
4. Adjust prompts to reduce edit distance
5. Keep fixtures growing over time to prevent regression
```

---

```markdown
===== FILE: docs/SECURITY_PRIVACY.md =====
# Security and privacy

## Data classification

- Audio and transcripts are sensitive
- Parent emails are personal data

## Controls in MVP

- Supabase RLS ensures users only see their own rows
- API verifies Supabase JWT and uses service role only server side
- Storage paths are per user namespace
- Encrypt in transit by default via HTTPS in production

## MVP limitations

- No formal GDPR DPA automation
- No enterprise compliance

## Immediate guardrails

- Default retention policy: delete audio after 30 days, configurable later
- Provide a delete lesson endpoint that also deletes storage object
```

---

```markdown
===== FILE: docs/DEPLOYMENT.md =====
# Deployment

## Supabase

- Create project
- Run migrations in supabase/migrations
- Create storage bucket: lesson-audio
- Configure RLS policies included in migration

## API

Recommended for MVP:
- Fly.io or Render
- Set env vars from .env.example
- Enable auto deploy from main branch

## Mobile

- Expo EAS build for iOS
- Store config in apps/mobile/app.json
- Set Supabase URL and anon key in app config

## Observability

- Optional Sentry DSN in API and mobile
- Log job durations and errors in jobs table
```

---

```markdown
===== FILE: docs/RUNBOOK.md =====
# Runbook

## Common failures

1. Whisper errors
- Symptom: job status FAILED at TRANSCRIBING
- Action: retry from TRANSCRIBE
- Check: file size and format

2. Schema validation errors
- Symptom: job FAILED at EXTRACT
- Action: inspect raw model output in job metadata
- Fix: prompt or schema mismatch

3. Output quality regressions
- Symptom: teachers report wrong plan
- Action: add a new golden fixture and tune prompts
- Confirm: CI is green on fixtures
```

---

```markdown
===== FILE: docs/PAYMENTS.md =====
# Payments strategy

The idea includes pricing tiers, but MVP should not block pilots behind Stripe until core quality is proven.

MVP approach:
- Invite code onboarding
- Plan is stored in profiles.plan with SOLO default
- Studio limits enforced by app and DB constraints

Post MVP:
- Stripe subscription
- Webhook updates profiles.plan and profiles.plan_status
```

---

```env
===== FILE: .env.example =====
# Supabase
SUPABASE_URL=
SUPABASE_ANON_KEY=
SUPABASE_SERVICE_ROLE_KEY=

# OpenAI
OPENAI_API_KEY=
OPENAI_TRANSCRIBE_MODEL=whisper-1
OPENAI_LLM_MODEL=gpt-4o-mini

# Email
RESEND_API_KEY=
EMAIL_FROM=notes@notebynote.app

# API
API_HOST=0.0.0.0
API_PORT=8000
```

---

```makefile
===== FILE: Makefile =====
.PHONY: api-install api-dev mobile-install mobile-dev test fmt lint ci

api-install:
	cd services/api && python -m pip install -U pip && pip install -e ".[dev]"

api-dev:
	cd services/api && uvicorn app.main:app --reload --host 0.0.0.0 --port 8000

mobile-install:
	cd apps/mobile && npm install

mobile-dev:
	cd apps/mobile && npm run start

test:
	cd services/api && pytest
	cd packages/ai_contract && pytest

fmt:
	cd services/api && ruff format .
	cd services/api && ruff check . --fix

lint:
	cd services/api && ruff check .
	cd services/api && mypy app

ci: lint test
```

---

```yaml
===== FILE: .github/workflows/ci.yml =====
name: CI

on:
  push:
    branches: [ main ]
  pull_request:

jobs:
  python:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: "3.12"
      - name: Install
        run: |
          python -m pip install -U pip
          pip install -e "services/api[dev]"
          pip install -e "packages/ai_contract[dev]"
      - name: Lint
        run: |
          cd services/api
          ruff check .
      - name: Tests
        run: |
          cd services/api
          pytest
          cd ../../packages/ai_contract
          pytest
```

---

```gitignore
===== FILE: .gitignore =====
.env
__pycache__/
*.pyc
.coverage
.coverage.*
dist/
node_modules/
.expo/
.DS_Store
```

---

```sql
===== FILE: supabase/migrations/001_init.sql =====
-- Extensions
create extension if not exists "uuid-ossp";

-- Profiles
create table if not exists public.profiles (
  id uuid primary key references auth.users(id) on delete cascade,
  created_at timestamptz not null default now(),
  plan text not null default 'SOLO',
  plan_status text not null default 'ACTIVE'
);

-- Students
create table if not exists public.students (
  id uuid primary key default uuid_generate_v4(),
  owner_id uuid not null references public.profiles(id) on delete cascade,
  created_at timestamptz not null default now(),
  name text not null,
  instrument text null,
  parent_email text null
);

-- Lessons
create table if not exists public.lessons (
  id uuid primary key default uuid_generate_v4(),
  owner_id uuid not null references public.profiles(id) on delete cascade,
  student_id uuid not null references public.students(id) on delete cascade,
  created_at timestamptz not null default now(),
  title text null,
  status text not null default 'QUEUED',
  audio_path text not null,
  transcript text null,
  error_code text null,
  error_message text null
);

-- Outputs
create table if not exists public.outputs (
  id uuid primary key default uuid_generate_v4(),
  owner_id uuid not null references public.profiles(id) on delete cascade,
  lesson_id uuid not null references public.lessons(id) on delete cascade,
  created_at timestamptz not null default now(),
  updated_at timestamptz not null default now(),
  type text not null,
  content text not null,
  edited_content text null,
  sent_at timestamptz null,
  sent_to text null,
  sent_via text null
);

-- Jobs
create table if not exists public.jobs (
  id uuid primary key default uuid_generate_v4(),
  owner_id uuid not null references public.profiles(id) on delete cascade,
  lesson_id uuid not null references public.lessons(id) on delete cascade,
  created_at timestamptz not null default now(),
  updated_at timestamptz not null default now(),
  step text not null default 'QUEUED',
  progress int not null default 0,
  last_error text null,
  attempts int not null default 0
);

-- Simple updated_at trigger
create or replace function public.set_updated_at()
returns trigger language plpgsql as $$
begin
  new.updated_at = now();
  return new;
end; $$;

drop trigger if exists trg_outputs_updated_at on public.outputs;
create trigger trg_outputs_updated_at
before update on public.outputs
for each row execute function public.set_updated_at();

drop trigger if exists trg_jobs_updated_at on public.jobs;
create trigger trg_jobs_updated_at
before update on public.jobs
for each row execute function public.set_updated_at();

-- RLS
alter table public.profiles enable row level security;
alter table public.students enable row level security;
alter table public.lessons enable row level security;
alter table public.outputs enable row level security;
alter table public.jobs enable row level security;

-- Policies
create policy "profiles_select_own" on public.profiles
for select using (auth.uid() = id);

create policy "students_crud_own" on public.students
for all using (auth.uid() = owner_id) with check (auth.uid() = owner_id);

create policy "lessons_crud_own" on public.lessons
for all using (auth.uid() = owner_id) with check (auth.uid() = owner_id);

create policy "outputs_crud_own" on public.outputs
for all using (auth.uid() = owner_id) with check (auth.uid() = owner_id);

create policy "jobs_crud_own" on public.jobs
for all using (auth.uid() = owner_id) with check (auth.uid() = owner_id);
```

---

```markdown
===== FILE: packages/ai_contract/README.md =====
# AI contract package

This package is the contract and regression harness for the AI layer.

What it guarantees:
- Extraction output matches a JSON schema
- Generation outputs match a JSON schema
- Golden fixtures prevent drift

Run:
- pytest
```

---

```json
===== FILE: packages/ai_contract/schema/lesson_extraction.schema.json =====
{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "title": "LessonExtraction",
  "type": "object",
  "additionalProperties": false,
  "required": ["student", "instrument", "highlights", "focus_areas", "assignments", "evidence"],
  "properties": {
    "student": { "type": "string", "minLength": 1 },
    "instrument": { "type": "string", "minLength": 1 },
    "highlights": { "type": "array", "items": { "type": "string" }, "minItems": 1 },
    "focus_areas": { "type": "array", "items": { "type": "string" }, "minItems": 1 },
    "assignments": {
      "type": "array",
      "minItems": 1,
      "items": {
        "type": "object",
        "additionalProperties": false,
        "required": ["task", "target", "confidence"],
        "properties": {
          "task": { "type": "string", "minLength": 1 },
          "target": { "type": "string", "minLength": 1 },
          "confidence": { "type": "number", "minimum": 0, "maximum": 1 }
        }
      }
    },
    "evidence": {
      "type": "array",
      "items": {
        "type": "object",
        "additionalProperties": false,
        "required": ["claim", "quote"],
        "properties": {
          "claim": { "type": "string", "minLength": 1 },
          "quote": { "type": "string", "minLength": 1 }
        }
      }
    }
  }
}
```

---

```json
===== FILE: packages/ai_contract/schema/outputs.schema.json =====
{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "title": "LessonOutputs",
  "type": "object",
  "additionalProperties": false,
  "required": ["student_recap", "practice_plan", "parent_email"],
  "properties": {
    "student_recap": { "type": "string", "minLength": 50 },
    "practice_plan": { "type": "string", "minLength": 100 },
    "parent_email": { "type": "string", "minLength": 50 }
  }
}
```

---

```markdown
===== FILE: packages/ai_contract/prompts/extraction.md =====
You are extracting structured lesson facts from a transcript of a music lesson.

Return only valid JSON that matches the provided JSON Schema.

Rules:
- Use evidence quotes directly from transcript where possible.
- If a field is unknown, infer conservatively and keep confidence low.
- Do not hallucinate repertoire names unless mentioned.

JSON Schema:
{{SCHEMA_JSON}}
```

---

```markdown
===== FILE: packages/ai_contract/prompts/student_recap.md =====
Write a student recap for the teacher to share with the student.

Constraints:
- 150 to 300 words
- Encouraging and specific
- Refer only to facts in the extraction JSON

Input:
{{EXTRACTION_JSON}}
```

---

```markdown
===== FILE: packages/ai_contract/prompts/practice_plan.md =====
Write a 7 day practice plan.

Constraints:
- Day 1 to Day 7 headings
- Each day includes time estimate and 2 to 4 concrete tasks
- Measurable targets when possible
- Refer only to facts in the extraction JSON

Input:
{{EXTRACTION_JSON}}
```

---

```markdown
===== FILE: packages/ai_contract/prompts/parent_email.md =====
Write a parent email that the teacher can send.

Constraints:
- Professional, warm, concise
- One subject line suggestion at top
- Mention highlights and next steps
- Refer only to facts in the extraction JSON

Input:
{{EXTRACTION_JSON}}
```

---

```text
===== FILE: packages/ai_contract/fixtures/golden/fixture_0001/transcript.txt =====
Teacher: Great work on the C major scale today, nice even tone.
Student: I keep rushing the left hand in the Bach piece.
Teacher: This week I want 10 minutes hands separate, slow tempo with metronome at 60.
Teacher: Also practice the G major arpeggio, two octaves.
```

---

```json
===== FILE: packages/ai_contract/fixtures/golden/fixture_0001/expected_extraction.json =====
{
  "student": "Unknown",
  "instrument": "Piano",
  "highlights": ["C major scale with even tone"],
  "focus_areas": ["Do not rush the left hand in the Bach piece"],
  "assignments": [
    { "task": "Hands separate practice on Bach passage", "target": "10 minutes daily at metronome 60", "confidence": 0.8 },
    { "task": "G major arpeggio", "target": "Two octaves daily", "confidence": 0.7 }
  ],
  "evidence": [
    { "claim": "Practice hands separate at tempo 60", "quote": "10 minutes hands separate, slow tempo with metronome at 60" },
    { "claim": "Practice G major arpeggio two octaves", "quote": "practice the G major arpeggio, two octaves" }
  ]
}
```

---

```json
===== FILE: packages/ai_contract/fixtures/golden/fixture_0001/expected_outputs.json =====
{
  "student_recap": "Placeholder recap used for deterministic tests.",
  "practice_plan": "Placeholder plan used for deterministic tests.",
  "parent_email": "Placeholder email used for deterministic tests."
}
```

---

```python
===== FILE: packages/ai_contract/src/adapters.py =====
from __future__ import annotations

from dataclasses import dataclass
from typing import Protocol


@dataclass(frozen=True)
class AdapterResult:
    text: str


class LLMAdapter(Protocol):
    def complete(self, prompt: str) -> AdapterResult: ...


class DeterministicAdapter:
    """
    Used in CI and fixture tests.
    Returns deterministic outputs so golden fixtures can validate shape and plumbing.
    """

    def __init__(self, mapping: dict[str, str]) -> None:
        self.mapping = mapping

    def complete(self, prompt: str) -> AdapterResult:
        for key, value in self.mapping.items():
            if key in prompt:
                return AdapterResult(text=value)
        return AdapterResult(text='{}')
```

---

```python
===== FILE: packages/ai_contract/src/validate.py =====
from __future__ import annotations

import json
from pathlib import Path

import jsonschema


def load_schema(path: Path) -> dict:
    return json.loads(path.read_text(encoding="utf-8"))


def validate_json(instance: dict, schema: dict) -> None:
    jsonschema.validate(instance=instance, schema=schema)
```

---

```python
===== FILE: packages/ai_contract/src/runner.py =====
from __future__ import annotations

import json
from pathlib import Path

from .adapters import LLMAdapter
from .validate import load_schema, validate_json


ROOT = Path(__file__).resolve().parents[1]


def render_prompt(template_path: Path, replacements: dict[str, str]) -> str:
    text = template_path.read_text(encoding="utf-8")
    for k, v in replacements.items():
        text = text.replace(k, v)
    return text


def extract(adapter: LLMAdapter, transcript: str) -> dict:
    schema_path = ROOT / "schema" / "lesson_extraction.schema.json"
    prompt_path = ROOT / "prompts" / "extraction.md"
    schema_json = schema_path.read_text(encoding="utf-8")

    prompt = render_prompt(
        prompt_path,
        {
            "{{SCHEMA_JSON}}": schema_json,
        },
    ) + "\n\nTRANSCRIPT:\n" + transcript

    raw = adapter.complete(prompt).text
    data = json.loads(raw)

    validate_json(data, load_schema(schema_path))
    return data


def generate(adapter: LLMAdapter, extraction_json: dict) -> dict:
    schema_path = ROOT / "schema" / "outputs.schema.json"
    schema = load_schema(schema_path)
    extraction_str = json.dumps(extraction_json, ensure_ascii=False)

    def gen_one(prompt_file: str) -> str:
        p = ROOT / "prompts" / prompt_file
        prompt = render_prompt(p, {"{{EXTRACTION_JSON}}": extraction_str})
        return adapter.complete(prompt).text

    outputs = {
        "student_recap": gen_one("student_recap.md"),
        "practice_plan": gen_one("practice_plan.md"),
        "parent_email": gen_one("parent_email.md"),
    }

    validate_json(outputs, schema)
    return outputs
```

---

```python
===== FILE: packages/ai_contract/tests/test_golden_fixtures.py =====
from __future__ import annotations

import json
from pathlib import Path

from ai_contract.src.adapters import DeterministicAdapter
from ai_contract.src.runner import extract, generate


ROOT = Path(__file__).resolve().parents[1]


def test_fixture_0001_shapes() -> None:
    fixture = ROOT / "fixtures" / "golden" / "fixture_0001"
    transcript = (fixture / "transcript.txt").read_text(encoding="utf-8")

    expected_extraction = json.loads((fixture / "expected_extraction.json").read_text(encoding="utf-8"))
    expected_outputs = json.loads((fixture / "expected_outputs.json").read_text(encoding="utf-8"))

    adapter = DeterministicAdapter(
        mapping={
            "TRANSCRIPT:": json.dumps(expected_extraction),
            "Write a student recap": expected_outputs["student_recap"],
            "Write a 7 day practice plan": expected_outputs["practice_plan"],
            "Write a parent email": expected_outputs["parent_email"],
        }
    )

    got_extraction = extract(adapter, transcript)
    got_outputs = generate(adapter, got_extraction)

    assert set(got_extraction.keys()) == set(expected_extraction.keys())
    assert set(got_outputs.keys()) == set(expected_outputs.keys())
```

---

```toml
===== FILE: services/api/pyproject.toml =====
[project]
name = "notebynote-api"
version = "0.1.0"
requires-python = ">=3.12"
dependencies = [
  "fastapi>=0.110",
  "uvicorn[standard]>=0.30",
  "pydantic>=2.7",
  "pydantic-settings>=2.2",
  "requests>=2.32",
  "supabase>=2.4.0",
  "openai>=1.30.0",
]

[project.optional-dependencies]
dev = [
  "pytest>=8.0",
  "ruff>=0.5",
  "mypy>=1.10",
]

[tool.ruff]
line-length = 100

[tool.mypy]
python_version = "3.12"
strict = true
```

---

```python
===== FILE: services/api/app/settings.py =====
from __future__ import annotations

from pydantic_settings import BaseSettings


class Settings(BaseSettings):
    supabase_url: str
    supabase_anon_key: str
    supabase_service_role_key: str

    openai_api_key: str
    openai_transcribe_model: str = "whisper-1"
    openai_llm_model: str = "gpt-4o-mini"

    resend_api_key: str | None = None
    email_from: str | None = None

    class Config:
        env_file = ".env"


settings = Settings()
```

---

```python
===== FILE: services/api/app/auth.py =====
from __future__ import annotations

import requests

from .errors import AppError
from .settings import settings


def verify_supabase_token(access_token: str) -> str:
    """
    Minimal MVP verification:
    - Call Supabase auth user endpoint with the bearer token
    - Return user id
    """
    url = f"{settings.supabase_url}/auth/v1/user"
    headers = {
        "Authorization": f"Bearer {access_token}",
        "apikey": settings.supabase_anon_key,
    }
    resp = requests.get(url, headers=headers, timeout=10)
    if resp.status_code != 200:
        raise AppError(code="AUTH_INVALID", message="Invalid or expired token")
    data = resp.json()
    user_id = data.get("id")
    if not user_id:
        raise AppError(code="AUTH_INVALID", message="Missing user id")
    return user_id
```

---

```python
===== FILE: services/api/app/errors.py =====
from __future__ import annotations

from dataclasses import dataclass


@dataclass
class AppError(Exception):
    code: str
    message: str
```

---

```python
===== FILE: services/api/app/db.py =====
from __future__ import annotations

from supabase import create_client, Client

from .settings import settings


def supabase_service() -> Client:
    return create_client(settings.supabase_url, settings.supabase_service_role_key)
```

---

```python
===== FILE: services/api/app/models.py =====
from __future__ import annotations

from pydantic import BaseModel, Field


class Envelope(BaseModel):
    success: bool = True
    data: dict


class ErrorEnvelope(BaseModel):
    success: bool = False
    error: dict


class CreateLessonRequest(BaseModel):
    studentId: str
    title: str | None = None
    audioStoragePath: str = Field(min_length=1)


class LessonStatusResponse(BaseModel):
    lessonId: str
    status: str
    step: str
    progress: int
    lastError: str | None = None
```

---

```python
===== FILE: services/api/app/services/openai_client.py =====
from __future__ import annotations

from openai import OpenAI

from ..settings import settings


def client() -> OpenAI:
    return OpenAI(api_key=settings.openai_api_key)
```

---

```python
===== FILE: services/api/app/services/ai_pipeline.py =====
from __future__ import annotations

import json
from pathlib import Path

from openai import OpenAI

from packages.ai_contract.src.validate import validate_json, load_schema

AI_ROOT = Path(__file__).resolve().parents[3] / "packages" / "ai_contract"


def _load_prompt(name: str) -> str:
    return (AI_ROOT / "prompts" / name).read_text(encoding="utf-8")


def transcribe(oai: OpenAI, audio_file_path: str) -> str:
    with open(audio_file_path, "rb") as f:
        result = oai.audio.transcriptions.create(
            model="whisper-1",
            file=f,
        )
    return result.text


def extract(oai: OpenAI, transcript: str) -> dict:
    schema_path = AI_ROOT / "schema" / "lesson_extraction.schema.json"
    prompt = _load_prompt("extraction.md").replace(
        "{{SCHEMA_JSON}}", schema_path.read_text(encoding="utf-8")
    )
    prompt = prompt + "\n\nTRANSCRIPT:\n" + transcript

    res = oai.chat.completions.create(
        model="gpt-4o-mini",
        response_format={"type": "json_object"},
        messages=[{"role": "user", "content": prompt}],
    )
    raw = res.choices[0].message.content or "{}"
    data = json.loads(raw)

    validate_json(data, load_schema(schema_path))
    return data


def generate(oai: OpenAI, extraction_json: dict) -> dict:
    schema_path = AI_ROOT / "schema" / "outputs.schema.json"
    schema = load_schema(schema_path)
    extraction_str = json.dumps(extraction_json, ensure_ascii=False)

    def run_one(prompt_name: str) -> str:
        prompt = _load_prompt(prompt_name).replace("{{EXTRACTION_JSON}}", extraction_str)
        res = oai.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
        )
        return (res.choices[0].message.content or "").strip()

    outputs = {
        "student_recap": run_one("student_recap.md"),
        "practice_plan": run_one("practice_plan.md"),
        "parent_email": run_one("parent_email.md"),
    }
    validate_json(outputs, schema)
    return outputs
```

---

```python
===== FILE: services/api/app/services/emailer.py =====
from __future__ import annotations

from ..settings import settings


def can_send() -> bool:
    return bool(settings.resend_api_key and settings.email_from)


def build_mailto(to: str, subject: str, body: str) -> str:
    import urllib.parse
    return f"mailto:{urllib.parse.quote(to)}?subject={urllib.parse.quote(subject)}&body={urllib.parse.quote(body)}"
```

---

```python
===== FILE: services/api/app/routes/health.py =====
from __future__ import annotations

from fastapi import APIRouter

router = APIRouter()


@router.get("/health")
def health() -> dict:
    return {"success": True, "data": {"status": "ok"}}
```

---

```python
===== FILE: services/api/app/routes/students.py =====
from __future__ import annotations

from fastapi import APIRouter, Header

from ..auth import verify_supabase_token
from ..db import supabase_service
from ..errors import AppError

router = APIRouter(prefix="/v1/students", tags=["students"])


@router.get("")
def list_students(authorization: str = Header(...)) -> dict:
    token = authorization.replace("Bearer ", "")
    user_id = verify_supabase_token(token)

    sb = supabase_service()
    res = sb.table("students").select("*").eq("owner_id", user_id).order("created_at", desc=True).execute()
    return {"success": True, "data": {"students": res.data}}


@router.post("")
def create_student(payload: dict, authorization: str = Header(...)) -> dict:
    token = authorization.replace("Bearer ", "")
    user_id = verify_supabase_token(token)

    name = (payload.get("name") or "").strip()
    if not name:
        raise AppError(code="VALIDATION", message="Student name required")

    sb = supabase_service()
    res = sb.table("students").insert(
        {
            "owner_id": user_id,
            "name": name,
            "instrument": payload.get("instrument"),
            "parent_email": payload.get("parent_email"),
        }
    ).execute()
    return {"success": True, "data": {"student": res.data[0]}}
```

---

```python
===== FILE: services/api/app/routes/lessons.py =====
from __future__ import annotations

import tempfile
from fastapi import APIRouter, Header

from ..auth import verify_supabase_token
from ..db import supabase_service
from ..errors import AppError
from ..models import CreateLessonRequest
from ..services.openai_client import client as openai_client
from ..services.ai_pipeline import transcribe, extract, generate

router = APIRouter(prefix="/v1/lessons", tags=["lessons"])


@router.post("")
def create_lesson(req: CreateLessonRequest, authorization: str = Header(...)) -> dict:
    token = authorization.replace("Bearer ", "")
    user_id = verify_supabase_token(token)

    sb = supabase_service()

    lesson = sb.table("lessons").insert(
        {
            "owner_id": user_id,
            "student_id": req.studentId,
            "title": req.title,
            "status": "TRANSCRIBING",
            "audio_path": req.audioStoragePath,
        }
    ).execute().data[0]

    job = sb.table("jobs").insert(
        {"owner_id": user_id, "lesson_id": lesson["id"], "step": "TRANSCRIBING", "progress": 5}
    ).execute().data[0]

    # MVP pipeline runs inline for simplicity.
    # For longer audio, migrate this to a worker without changing API contract.

    try:
        oai = openai_client()

        # Downloading audio from storage is environment specific.
        # In MVP, expect the client to also upload a short proxy file to the API in a later iteration.
        # For now, we fail fast if we cannot access the file.
        raise AppError(code="NOT_IMPLEMENTED", message="Audio fetch from storage not wired yet")

    except AppError as e:
        sb.table("lessons").update({"status": "FAILED", "error_code": e.code, "error_message": e.message}).eq(
            "id", lesson["id"]
        ).execute()
        sb.table("jobs").update({"step": "FAILED", "progress": 100, "last_error": e.message}).eq("id", job["id"]).execute()
        return {"success": False, "error": {"code": e.code, "message": e.message}}

    except Exception as e:
        msg = str(e)
        sb.table("lessons").update({"status": "FAILED", "error_code": "UNKNOWN", "error_message": msg}).eq(
            "id", lesson["id"]
        ).execute()
        sb.table("jobs").update({"step": "FAILED", "progress": 100, "last_error": msg}).eq("id", job["id"]).execute()
        return {"success": False, "error": {"code": "UNKNOWN", "message": msg}}


@router.get("/{lesson_id}/status")
def lesson_status(lesson_id: str, authorization: str = Header(...)) -> dict:
    token = authorization.replace("Bearer ", "")
    user_id = verify_supabase_token(token)

    sb = supabase_service()
    lesson = sb.table("lessons").select("*").eq("id", lesson_id).eq("owner_id", user_id).single().execute().data
    job = sb.table("jobs").select("*").eq("lesson_id", lesson_id).eq("owner_id", user_id).single().execute().data

    return {
        "success": True,
        "data": {
            "lessonId": lesson_id,
            "status": lesson["status"],
            "step": job["step"],
            "progress": job["progress"],
            "lastError": job.get("last_error"),
        },
    }
```

---

```python
===== FILE: services/api/app/routes/outputs.py =====
from __future__ import annotations

from fastapi import APIRouter, Header

from ..auth import verify_supabase_token
from ..db import supabase_service
from ..services.emailer import can_send, build_mailto

router = APIRouter(prefix="/v1/outputs", tags=["outputs"])


@router.patch("/{output_id}")
def update_output(output_id: str, payload: dict, authorization: str = Header(...)) -> dict:
    token = authorization.replace("Bearer ", "")
    user_id = verify_supabase_token(token)

    edited = payload.get("editedContent")
    sb = supabase_service()
    res = sb.table("outputs").update({"edited_content": edited}).eq("id", output_id).eq("owner_id", user_id).execute()
    return {"success": True, "data": {"output": res.data[0]}}


@router.post("/{output_id}/sent")
def mark_sent(output_id: str, payload: dict, authorization: str = Header(...)) -> dict:
    token = authorization.replace("Bearer ", "")
    user_id = verify_supabase_token(token)

    sb = supabase_service()
    res = sb.table("outputs").update(
        {"sent_to": payload.get("sentTo"), "sent_via": payload.get("sentVia"), "sent_at": "now()"}
    ).eq("id", output_id).eq("owner_id", user_id).execute()
    return {"success": True, "data": {"output": res.data[0]}}


@router.post("/{output_id}/send-email")
def send_email(output_id: str, payload: dict, authorization: str = Header(...)) -> dict:
    """
    MVP behavior:
    - If Resend configured, later implement real send.
    - Otherwise return a mailto link the app can open.
    """
    token = authorization.replace("Bearer ", "")
    user_id = verify_supabase_token(token)

    sb = supabase_service()
    out = sb.table("outputs").select("*").eq("id", output_id).eq("owner_id", user_id).single().execute().data

    to = payload.get("to") or out.get("sent_to")
    subject = "Lesson summary"
    body = (out.get("edited_content") or out.get("content") or "").strip()

    if not can_send():
        return {"success": True, "data": {"method": "mailto", "mailto": build_mailto(to, subject, body)}}

    return {"success": False, "error": {"code": "NOT_IMPLEMENTED", "message": "Resend integration stub"}}
```

---

```python
===== FILE: services/api/app/main.py =====
from __future__ import annotations

from fastapi import FastAPI
from fastapi.responses import JSONResponse

from .errors import AppError
from .routes.health import router as health_router
from .routes.students import router as students_router
from .routes.lessons import router as lessons_router
from .routes.outputs import router as outputs_router

app = FastAPI(title="Note By Note API", version="0.1.0")

app.include_router(health_router)
app.include_router(students_router)
app.include_router(lessons_router)
app.include_router(outputs_router)


@app.exception_handler(AppError)
def app_error_handler(_, exc: AppError) -> JSONResponse:
    return JSONResponse(status_code=400, content={"success": False, "error": {"code": exc.code, "message": exc.message}})
```

---

```python
===== FILE: services/api/tests/test_health.py =====
from __future__ import annotations

from fastapi.testclient import TestClient

from app.main import app


def test_health() -> None:
    client = TestClient(app)
    resp = client.get("/health")
    assert resp.status_code == 200
    assert resp.json()["data"]["status"] == "ok"
```

---

```python
===== FILE: services/api/tests/test_pipeline_contract.py =====
from __future__ import annotations

import json

from packages.ai_contract.src.validate import validate_json, load_schema


def test_outputs_schema_validates() -> None:
    schema = load_schema(__import__("pathlib").Path("packages/ai_contract/schema/outputs.schema.json"))
    sample = {
        "student_recap": "x" * 60,
        "practice_plan": "y" * 120,
        "parent_email": "z" * 60,
    }
    validate_json(sample, schema)
```

---

```json
===== FILE: apps/mobile/package.json =====
{
  "name": "notebynote-mobile",
  "private": true,
  "version": "0.1.0",
  "main": "src/App.tsx",
  "scripts": {
    "start": "expo start",
    "typecheck": "tsc -p tsconfig.json --noEmit"
  },
  "dependencies": {
    "expo": "^51.0.0",
    "react": "18.2.0",
    "react-native": "0.74.0",
    "@supabase/supabase-js": "^2.45.0"
  },
  "devDependencies": {
    "typescript": "^5.5.0"
  }
}
```

---

```json
===== FILE: apps/mobile/app.json =====
{
  "expo": {
    "name": "Note By Note",
    "slug": "notebynote",
    "scheme": "notebynote",
    "platforms": ["ios", "android", "web"]
  }
}
```

---

```json
===== FILE: apps/mobile/tsconfig.json =====
{
  "compilerOptions": {
    "target": "ES2020",
    "module": "ESNext",
    "jsx": "react",
    "strict": true,
    "skipLibCheck": true
  }
}
```

---

```tsx
===== FILE: apps/mobile/src/App.tsx =====
import React, { useState } from "react";
import { SafeAreaView, View, Button, Text } from "react-native";
import RecordScreen from "./screens/RecordScreen";
import ReviewScreen from "./screens/ReviewScreen";

export default function App() {
  const [screen, setScreen] = useState<"record" | "review">("record");
  const [lessonId, setLessonId] = useState<string | null>(null);

  return (
    <SafeAreaView style={{ flex: 1, backgroundColor: "#000" }}>
      {screen === "record" && (
        <RecordScreen
          onCreated={(id) => {
            setLessonId(id);
            setScreen("review");
          }}
        />
      )}
      {screen === "review" && lessonId && (
        <ReviewScreen
          lessonId={lessonId}
          onBack={() => setScreen("record")}
        />
      )}
      <View style={{ padding: 12 }}>
        <Text style={{ color: "#888" }}>MVP navigation placeholder</Text>
      </View>
    </SafeAreaView>
  );
}
```

---

```ts
===== FILE: apps/mobile/src/lib/api.ts =====
export async function apiGet(path: string, token: string) {
  const res = await fetch(`http://localhost:8000${path}`, {
    headers: { Authorization: `Bearer ${token}` }
  });
  return res.json();
}

export async function apiPost(path: string, token: string, body: any) {
  const res = await fetch(`http://localhost:8000${path}`, {
    method: "POST",
    headers: {
      Authorization: `Bearer ${token}`,
      "Content-Type": "application/json"
    },
    body: JSON.stringify(body)
  });
  return res.json();
}
```

---

```ts
===== FILE: apps/mobile/src/lib/supabase.ts =====
import { createClient } from "@supabase/supabase-js";

// In MVP, inject via env or app config.
const SUPABASE_URL = process.env.EXPO_PUBLIC_SUPABASE_URL as string;
const SUPABASE_ANON_KEY = process.env.EXPO_PUBLIC_SUPABASE_ANON_KEY as string;

export const supabase = createClient(SUPABASE_URL, SUPABASE_ANON_KEY);
```

---

```tsx
===== FILE: apps/mobile/src/screens/RecordScreen.tsx =====
import React, { useState } from "react";
import { View, Button, Text } from "react-native";
import { supabase } from "../lib/supabase";
import { apiPost } from "../lib/api";

export default function RecordScreen(props: { onCreated: (lessonId: string) => void }) {
  const [status, setStatus] = useState<string>("idle");

  async function createLessonStub() {
    setStatus("creating");

    const session = (await supabase.auth.getSession()).data.session;
    if (!session) {
      setStatus("auth required");
      return;
    }

    // MVP stub: audio upload wiring comes next.
    const resp = await apiPost("/v1/lessons", session.access_token, {
      studentId: "REPLACE_ME",
      title: "Lesson",
      audioStoragePath: "REPLACE_ME"
    });

    if (!resp.success) {
      setStatus(resp.error?.message || "error");
      return;
    }

    props.onCreated(resp.data.lessonId);
  }

  return (
    <View style={{ padding: 16 }}>
      <Text style={{ color: "#fff", fontSize: 22, marginBottom: 12 }}>Record</Text>
      <Button title="Create lesson and process (stub)" onPress={createLessonStub} />
      <Text style={{ color: "#888", marginTop: 12 }}>{status}</Text>
    </View>
  );
}
```

---

```tsx
===== FILE: apps/mobile/src/screens/ReviewScreen.tsx =====
import React, { useEffect, useState } from "react";
import { View, Text, Button } from "react-native";
import { supabase } from "../lib/supabase";
import { apiGet } from "../lib/api";

export default function ReviewScreen(props: { lessonId: string; onBack: () => void }) {
  const [status, setStatus] = useState<any>(null);

  async function refresh() {
    const session = (await supabase.auth.getSession()).data.session;
    if (!session) return;
    const resp = await apiGet(`/v1/lessons/${props.lessonId}/status`, session.access_token);
    setStatus(resp);
  }

  useEffect(() => {
    refresh();
  }, []);

  return (
    <View style={{ padding: 16 }}>
      <Text style={{ color: "#fff", fontSize: 22, marginBottom: 12 }}>Review</Text>
      <Button title="Refresh status" onPress={refresh} />
      <Text style={{ color: "#888", marginTop: 12 }}>
        {JSON.stringify(status, null, 2)}
      </Text>
      <Button title="Back" onPress={props.onBack} />
    </View>
  );
}
```

